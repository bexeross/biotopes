---
title: "Specifications for distribution models of vulnerable marine ecosystem indicator assemblanges"
author: Genoveva Gonzalez Mirelis, Rebecca Ross, PÃ¥l Buhl-Mortensen
output: 
  html_notebook: 
    toc: yes
---

## Introduction
In compliance with the MAREANO Activity Plan, we have produced data and models to help towards the management of Vulnerable Marine Ecosystems (VMEs) and/or vulnerable biotopes. In this sheet we summarize all the specifications (code and outputs) for the models we have built that predict the distribution of VME indicator species/taxa/assemblages.

## Libraries
Load necessary libraries
```{r warning=FALSE}
library(raster)
library(rgdal)
library(partykit)
library(party)
library(tidyverse)
library(knitr)
library(spatstat)
library(maptools)
library(usdm)
library(vegan)
library(glmnet)
library(groupdata2)
```

## Predictors
Take a glance at the predictors
```{r}
kable(data.frame(Name = names(pred), Min_val = minValue(pred), Max_val = maxValue(pred)), format = "markdown", digits = 2)
```

## Potential responses
Take a glance at the VME indicator taxa, and how they group into what we have called assemblages
```{r}
kable(sppgrp)
```

## Strength of association to biotopes
Strength of association between VME indicator assemblages and MAREANO biotopes
```{r}
kable(indval3,format = "markdown", digits = 2)
```

### Summarized associations
```{r}
kable(indicbiot, format = "markdown", digits =2)
```

## Make the response into a spatial object
### Choose an indicator assemblage
```{r}
respv = c("30","31")
resp = paste0(respv)
resp = sapply(list(resp),function (x) base::paste(x, sep = " ",collapse=""))
as.character(with(sppgrp, species[spclass%in%respv]))
```

### Make it into a spatial object
```{r}

resp_spat <- respdf_sum %>%
  rownames_to_column(var = "SampID")%>%
  left_join(sample_info)%>%
  left_join(spe5) %>%
  select(SampID, resp, count, x_coord, y_coord)
resp_spat <-  SpatialPointsDataFrame(coords = resp_spat[,c(4,5)],
                                      data = resp_spat,
                                      proj4string = CRS("+proj=utm +zone=33 +datum=WGS84 +units=m +no_defs"))
resp_spat@data %>% select(contains(resp)) %>% pull %>% quantile(.,probs = seq(0, 1, 0.1))
```

## Reduce the dataset
Some species/taxa have very limited distributions. It may be a good idea to eliminate absence points that are too far away from any presence point
```{r}
## Select a subset of stations from the core distribution area(s) (code by RS)

# Step 1 - split into presence and absence points
abs<- subset(resp_spat, X3031==0)
pres<- subset(resp_spat, X3031>0)

# prevalence check
prev_1 <- length(pres)/length(resp_spat)

# Step 2 - find the distance to the nearest presence point of each absence point
absppp<- as(abs, "ppp")
presppp<-as(pres, "ppp")

dists<-nncross(absppp, presppp) # gives column dist (distance) and which (# of nearest presence point)

## if you want to check the distribution
#dist_ord<-dists[order(dists$dist),]
#plot(dist_ord$dist)

# Step 3 - find quartiles
qdist<-quantile(dists$dist,probs = seq(0, 1, 1/2)) # can modify the probs to change the number of divisions
lowQ<-qdist[[1]] # do not eliminate close ones
hiQ<-qdist[[2]]

# Step 4 - subset absences to exclude extreme quartiles
sel<-which((dists$dist>lowQ) & (dists$dist<hiQ))
sel.abs<-abs[sel, ]

## OUTPUT 
# Step 5 - combine with presences into output shapefile
resp_spat_sel<-rbind(pres,sel.abs)

# (prevalence check)
prev2<- length(pres)/length(resp_spat_sel)

prev2
```

## Extract values
```{r}
r <- resp_spat_sel
#r <- resp_spat

e <- raster::extract(pred, r)
v <- e %>% data.frame(cbind(r)) %>% mutate(landscape = factor(landscape)) %>% select(-contains("coord")) %>%
  transform(count=replace(count, which(count==0),1))
str(v)
```

## Variable selection
I don't think this is very important right now - all I care about is spatial predictions - but oh well
```{r}
fmla0 <- as.formula(paste(paste0("X",resp,"~"), 
                         paste(colnames(v)[1:(which(colnames(v)=="X")-1)], collapse= "+"))) # all predictors
m0 <- cforest(fmla0,
                      data=v, # all observations
                      #weights = v$count,
                      control = cforest_unbiased(ntree=1000,mtry = 3))
fmla0
```

### Check which variables are important at explaining presence/absence
```{r}
# Make a predictor-only matrix and a response-only vector, and also, leave only complete cases in there
x0 <- v %>% select(-c(paste0("X",resp), optional)) %>%
  select(-count) %>%
  select(-block)%>%
  filter(complete.cases(.))
cc <- x0$SampID

y <- v %>% filter(SampID %in% cc) %>%
  select(paste0("X",resp)) %>%
  pull()
  
y <- as_factor(decostand(y,method="pa"))

x <- x0 %>% select(-SampID) %>% data.matrix()

# Model coefficients
cvfit = cv.glmnet(x, y,family="binomial",type.measure="auc")
coef(cvfit, s = "lambda.1se")
```

### Check which variables are correlated to each other
```{r}
vset <- vifstep(data.frame(x), th=12)
vset@excluded
```

### Check overall variable importance of remaining variables
```{r}
x_new <- x %>% 
  data.frame() %>%
  select(-vset@excluded) %>%
  data.matrix()

fmla1 <- as.formula(paste(paste0("X",resp,"~"), 
                          paste(colnames(x_new), collapse= "+")))

dotplot(sort(varimp(cforest(fmla1,
                            data=v, # all observations 
                            #weights = v$count,
                            control = cforest_unbiased(ntree=1000,mtry = 3)))), xlab="Variable Importance", panel = function(x,y){ 
  panel.dotplot(x, y, col='darkblue', pch=16, cex=1.1) 
  panel.abline(v=abs(min(vi)), col='red', 
               lty='longdash', lwd=2
  )
})  
```
## Best model
A bit manual. Select the top variables according to variable importance making sure that they include the ones selected by glmnet (lasso?). This is the best because it's the simplest
```{r}
var <- sort(varimp(cforest(fmla1,
                            data=v, # all observations 
                           #weights = v$count,
                            control = cforest_unbiased(ntree=1000,mtry = 3))),decreasing=TRUE)
fmla2 <- as.formula(paste(paste0("X",resp,"~"), 
                          paste(names(var[1:9]), collapse= "+")))
fmla2
```


## Cross-validate with blocks
I have to get the spm library to work and replace rmse with vecv...
```{r}
v <- v %>% mutate(block = as_factor(sub("_.*", "", SampID)))

# Split data in 20/80 (percentage)
parts <- partition(v, p = 0.3, id_col = "block")
test_set <- parts[[1]]
train_set <- parts[[2]]

train_set <- fold(train_set, k = 4, id_col = 'block')

# Order by .folds
train_set <- train_set %>% arrange(.folds)
train_set <- train_set %>% arrange(.folds)

crossvalidate <- function(data, k, model, dependent){
  require(Metrics)
  # 'data' is the training set with the ".folds" column
  # 'k' is the number of folds we have
  # 'model' is a string describing a linear regression model formula
  # 'dependent' is a string with the name of the score column we want to predict
  # 'random' is a logical (TRUE/FALSE); do we have random effects in the model?
  # Initialize empty list for recording performances
  performances <- c()
  # One iteration per fold
  for (fold in 1:k){
    # Create training set for this iteration
    # Subset all the datapoints where .folds does not match the current fold
    training_set <- data[data$.folds != fold,]
    # Create test set for this iteration
    # Subset all the datapoints where .folds matches the current fold
    testing_set <- data[data$.folds == fold,]
    ## Train model
    # If there is a random effect,
    # use lmer() to train model
    # else use lm()
    modelmodel <- cforest(model, training_set, 
                          #weights = training_set$count,
                          control = cforest_unbiased(ntree=500,mtry = 3))
    ## Test model
    # Predict the dependent variable in the testing_set with the trained model
    predicted <- treeresponse(modelmodel, testing_set)
    # Get the Root Mean Square Error between the predicted and the observed
    RMSE <- rmse(unlist(predicted), testing_set[[dependent]])
    # Add the RMSE to the performance list
    performances[fold] <- RMSE
  }
  # Return the mean of the recorded RMSEs
  return(c('RMSE' = mean(performances)))
}

plot(c(crossvalidate(train_set,4,fmla0,"X3031"),
crossvalidate(train_set,4,fmla1,"X3031"),
crossvalidate(train_set,4,fmla2,"X3031"))~c(0,1,2), xlab = "model", ylab = "RMSE")
```

### Choose model
```{r}
fmlabest <- fmla1
```

## Export
```{r}
save(list=c("train_set", "test_set", "fmlabest"), file = file.path(outPath,"WORKSPACE_modeltraining.Rdata"))

save(list=c("train_set", "test_set", "fmlabest"), file = "WORKSPACE_modeltraining.Rdata") # save in working dir
```

## Calculate variance explained
This bit needs to be run on the server because I can't install the required package on my computer!
```{r}
library(spm)
load("WORKSPACE_modeltraining.Rdata")

data = train_set
model = cforest(fmlabest,
                data=data, 
                control = cforest_unbiased(ntree=1000,mtry = 3))
p <- treeresponse(model, test_set)
o <- test_set %>% select(contains(resp))
crosscheck <- data.frame(predicted = unlist(p), observed = pull(o))
ve <- vecv(crosscheck$observed,crosscheck$predicted)
ve
```
